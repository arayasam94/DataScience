{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Process of Data Science - Summer 2019\n",
    "## Assignment 1 \n",
    "\n",
    "\n",
    "### Submitted by\n",
    "\n",
    "Anirudh Rayasam\n",
    "Arjun Chandra Balaji Balaraman\n",
    "\n",
    "July, 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Explain the dataset. Focus on the attributes’ description.\n",
    "\n",
    "The dataset, Zomato Bangalore Restaurants, is a large data set containing information about the restaurants in the city. Various attributes of restaurants, like their localities, the type of food they serve, the approximate cost of food, how popular is the restaurant etc., have been provided. Understanding these attributes and using these for predictions will help us gain valuable knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Distribution of attributes based on frequency and finding trends in the data.\n",
    "\n",
    "The complete raw dataset was loaded into a Jupyter Notebook for exploring and understanding different attributes and their trends. The dataset contains many attributes out of which, few attributes seemed important and contained information that would be useful for predicting different things and gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing pandas library and reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WVCX3Lpd9yK"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"zomato.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially skimmed through the records (rows) and found that a lot of records only contained garbage value or are either completely empty. We filtered all such records using the ‘url’ attribute and deleted them using pandas commands. Now, we were left only with records which had valid data in them. This was an important step and this dataframe served as the base file for further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zdbhEgneFmz"
   },
   "outputs": [],
   "source": [
    "# remove records that doesn't have a valid url\n",
    "df1 = df[df['url'].astype(str).str.startswith('https')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed all the duplicate records from the dataframe. Duplicate records were identified using a combination of the attributes – ‘Address’, ’Name’ and ’Location’ as these three attributes will sufficiently remove all duplicate restaurants from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2DKm9eDeLzF"
   },
   "outputs": [],
   "source": [
    "# drop duplicates based on location, name, address alone\n",
    "ff = df1.drop_duplicates(subset=[\"location\",\"name\",\"address\"], keep=\"first\")\n",
    "copy_ff = ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are dropping all the columns that are not required for our prediction. This leaves us with five columns – location, rate, rest_type, cuisines and approx_cost(for two people). We’re also renaming the columns appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SndMcZggu_u"
   },
   "outputs": [],
   "source": [
    "# taking only required columns\n",
    "ff1 = ff[[\"name\",\"location\",\"rate\",\"rest_type\",\"cuisines\",\"approx_cost(for two people)\"]]\n",
    "ff1[\"cost\"] = ff1[\"approx_cost(for two people)\"]\n",
    "del ff1['approx_cost(for two people)']\n",
    "print(ff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rate attribute is then cleaned. All the records which has either a ‘-‘, ‘NEW’ or empty value in ‘rate’ attribute are removed as these do not provide useful insights. At the same time, we are splitting the rating attribute on ‘/’ to get the actual rating value. A float value ranging between 1.0 and 5.0. Formatting ‘cost’ column is also performed.\n",
    "\n",
    "Also, all the \\r escape characters are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "sTsWb0xVh1fn",
    "outputId": "e0d211b4-a9ca-4bbe-be7d-3afe2dae20a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4042: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6586: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3391: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "# remove \\r from all records\n",
    "ff1.replace(\"\\r\",\"\",regex=True,inplace=True)\n",
    "\n",
    "# remove blank spaces in rate column\n",
    "ff1[\"rate\"].replace(\" \",\"\",regex=True,inplace=True)\n",
    "\n",
    "# remove comma in cost column\n",
    "ff1[\"cost\"].replace(\",\",\"\",regex=True,inplace=True)\n",
    "\n",
    "# drop records that has empty records\n",
    "ff2 = ff1.dropna()\n",
    "ff3 = ff2\n",
    "\n",
    "# split rate based on \"/\"\n",
    "ff3[['rate','full_rate']] = ff3['rate'].str.split('/',expand=True)\n",
    "del ff3[\"full_rate\"]\n",
    "dataset = ff3\n",
    "del dataset[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Selecting location with the highest attribute.\n",
    "\n",
    "The following command considers the locations and the number of restaurants in each location, and based on that, the average rating of restaurants in that location. Then, we sort in descending order to get the highest average rating.\n",
    "\n",
    "Lavelle Road was the winning location in this case, and here are few trends and characteristics we could find for this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hDjdHEpOm427",
    "outputId": "71f4d0b0-a605-47f4-b540-e52fc117a90b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 location  Avg_Rate\n",
      "50           Lavelle Road  4.073077\n",
      "74            Sankey Road  4.041667\n",
      "80         St. Marks Road  3.929412\n",
      "42  Koramangala 3rd Block  3.926316\n",
      "44  Koramangala 5th Block  3.905882\n",
      "12          Church Street  3.898148\n",
      "65       Race Course Road  3.862069\n",
      "67   Rajarajeshwari Nagar  3.850000\n",
      "70          Richmond Road  3.843939\n",
      "43  Koramangala 4th Block  3.833010\n",
      "69         Residency Road  3.829730\n",
      "51                MG Road  3.809412\n",
      "25             Hosur Road  3.800000\n",
      "71         Sadashiv Nagar  3.789286\n",
      "27            Indiranagar  3.784186\n",
      "31              Jayanagar  3.760656\n",
      "46  Koramangala 7th Block  3.758915\n",
      "28          Infantry Road  3.753846\n",
      "35           Kalyan Nagar  3.740957\n",
      "15        Cunningham Road  3.736170\n",
      "45  Koramangala 6th Block  3.732824\n",
      "85          Vasanth Nagar  3.730233\n",
      "54           Malleshwaram  3.727228\n",
      "76          Seshadripuram  3.723913\n",
      "39            Koramangala  3.716667\n",
      "82                 Ulsoor  3.713978\n",
      "38                Kengeri  3.700000\n",
      "90              Yelahanka  3.700000\n",
      "8            Brigade Road  3.675926\n",
      "47  Koramangala 8th Block  3.675862\n",
      "..                    ...       ...\n",
      "16                 Domlur  3.521622\n",
      "77           Shanti Nagar  3.514000\n",
      "64               RT Nagar  3.500000\n",
      "10         CV Raman Nagar  3.500000\n",
      "83            Uttarahalli  3.500000\n",
      "58               Nagawara  3.496471\n",
      "6               Bellandur  3.495720\n",
      "53               Majestic  3.495161\n",
      "91            Yeshwantpur  3.490625\n",
      "34          Kaggadasapura  3.490000\n",
      "48     Kumaraswamy Layout  3.485937\n",
      "30              Jalahalli  3.480000\n",
      "3       Bannerghatta Road  3.479083\n",
      "2               Banaswadi  3.475177\n",
      "19        Electronic City  3.475110\n",
      "17         East Bangalore  3.475000\n",
      "18                Ejipura  3.473529\n",
      "37        Kanakapura Road  3.470000\n",
      "78           Shivajinagar  3.469697\n",
      "79        South Bangalore  3.468421\n",
      "24                 Hennur  3.467391\n",
      "89          Wilson Garden  3.453125\n",
      "52            Magadi Road  3.418750\n",
      "87         West Bangalore  3.400000\n",
      "57             Nagarbhavi  3.400000\n",
      "60        North Bangalore  3.375000\n",
      "62        Old Madras Road  3.355556\n",
      "68        Rammurthy Nagar  3.353333\n",
      "7            Bommanahalli  3.216364\n",
      "63                 Peenya  3.200000\n",
      "\n",
      "[92 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# 2.d\n",
    "# select location with highest average rating\n",
    "# make location count for every unique values\n",
    "ff4 = ff3.convert_objects(convert_numeric=True)\n",
    "h_avg = ff4.groupby('location')['rate'].mean().reset_index(name='Avg_Rate')\n",
    "h_avg = h_avg.sort_values(by='Avg_Rate',ascending=False)\n",
    "print(h_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "vyRUTZtfyAji",
    "outputId": "f600433f-1285-4293-bd1d-ac757d530859",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location with highest avg rating: Lavelle Road\n",
      "Lavelle Road is famous for North Indian cuisines\n",
      "Lavelle Road is famous for Fine Dining restaurants\n",
      "Lavelle Road is famous for Fine Dining restaurants\n"
     ]
    }
   ],
   "source": [
    "# charac of high rating location\n",
    "h_location = h_avg['location'].iloc[0]\n",
    "cuisine_charac = pd.DataFrame()\n",
    "rest_charac = pd.DataFrame()\n",
    "\n",
    "print(\"location with highest avg rating:\",h_location)\n",
    "neigh = ff3.loc[df['location'] == h_location]\n",
    "\n",
    "for row in neigh[\"cuisines\"]:\n",
    "  cuisines = row.split(\",\")\n",
    "  cuisines = [x.strip(' ') for x in cuisines]\n",
    "  for i in cuisines:\n",
    "    cuisine_charac = cuisine_charac.append(pd.Series(i),ignore_index=True)\n",
    "    \n",
    "for j in neigh[\"rest_type\"]:\n",
    "  rest = j.split(\",\")\n",
    "  \n",
    "  rest = [x.strip(' ') for x in rest]\n",
    "  for k in rest:\n",
    "    rest_charac = rest_charac.append(pd.Series(k),ignore_index=True)\n",
    "    \n",
    "h_cuisine = cuisine_charac[0].iloc[0]\n",
    "h_rest = rest_charac[0].iloc[0]\n",
    "print(h_location,\"is famous for\",h_cuisine,\"cuisines\")\n",
    "print(h_location,\"is famous for\",h_rest,\"restaurants\")\n",
    "print(h_location,\"is famous for\",h_rest,\"restaurants\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing important libraries for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gslbMlAc_c3T"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting columns like cuisine and rest_type into individual, proper records as a part of cleaning\n",
    "\n",
    "We divide the multivalued columns like rest_type and cuisines, to individual records. This step is crucial in order to avoid duplicate values and gain proper ‘One Hot Encoding’. The list attribute has attributes stored in a way that can lead to duplicates. For Ex: consider these two records in ‘cuisines’\n",
    "\n",
    "1. Chinese, Continental, North Indian\n",
    "2. North Indian, Chinese, Fast Food, Biryani\n",
    "\n",
    "When the system parses the above two list of attributes, it will parse ‘Chinese’ from the first list, but ‘ Chinese’ (a space before C) from the second list. Now, the system may consider these two as two different categories and so, will encode them differently. This is incorrect and can cause incorrect encoding. \n",
    "\n",
    "Therefore, we are splitting each list value as an individual record and trimming it to remove spaces. While performing ‘One Hot Encoding’/’Dummy variables’ to convert categorical values as continuous ones, without considering values redundantly as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcDHNEymIdyF"
   },
   "outputs": [],
   "source": [
    "# split list columns into rows\n",
    "def splitColumnList(dataFrame, column):\n",
    "    newDataFrame = pd.DataFrame()\n",
    "    dataFrame[column] = dataFrame[column].str.split(',')\n",
    "    for index, row in dataFrame.iterrows():\n",
    "        for i in row[column]:\n",
    "            newColumn = \"new_\"+column\n",
    "            row[newColumn] = i\n",
    "            newDataFrame = newDataFrame.append(row,ignore_index=True)\n",
    "        print(index)\n",
    "    newDataFrame[column] = newDataFrame[\"new_\"+column]\n",
    "    newDataFrame.drop(\"new_\"+column, axis=1, inplace=True)\n",
    "    return newDataFrame\n",
    "  \n",
    "\n",
    "temp_dataset = splitColumnList(dataset,\"rest_type\")\n",
    "temp_dataset = splitColumnList(temp_dataset,\"cuisines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing spaces and dropping null values\n",
    "\n",
    "Cleaning the unnecesary spaces from attributes and dropping rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLouVwiLUOJw"
   },
   "outputs": [],
   "source": [
    "# temp_dataset.to_csv(\"clean_data.csv\")\n",
    "# print(temp_dataset)\n",
    "indexNames = temp_dataset[ temp_dataset['rate'] == '-' ].index\n",
    "temp_dataset.drop(indexNames , inplace=True)\n",
    "temp_dataset.dropna()\n",
    "temp_data['location'] = temp_data['location'].str.replace(' ', '')\n",
    "temp_data['rest_type'] = temp_data['rest_type'].str.replace(' ', '')\n",
    "temp_data['cuisines'] = temp_data['cuisines'].str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Explain what is the task you’re solving (e.g., supervised x unsupervised, classification x regression x clustering or similarity matching x etc.)? \n",
    "\n",
    "The given problem statement requires the prediction of approximate_cost for two people, based on few features. Since, we already know the label attribute (attribute that needs to be predicted), and that we have the same attribute values for our training as well, we are performing a supervised prediction. \n",
    "\n",
    "The cost is a continuous value and we consider Regression supervised modeling to predict continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. What models will you choose, and why?\n",
    "\n",
    "We’ve considered the following models to perform our regression.\n",
    "\n",
    "1.\tDecision Trees Regressor: Decision tree regressor is one of the most popular and efficient predictive modeling techniques for regressions. Few advantages of using this is, it doesn’t require much preparation for performing, and any non-linear relationships between the features will not affect its predictions. We are using the weighted mean square error method to choose the nodes and making sure that the model doesn’t go too deep and overfit.\n",
    "\n",
    "2.\tRandom Forest Regressor: Random Forest in an ensemble technique where it trains different decision trees, and combines the output to provide a more general prediction. Using Bagging method, different decision trees are created using different data samples. By combining the results in a meaningful manner, we can gain a model which does not overfit.\n",
    "\n",
    "3.\tXGBoost: Extreme Gradient Boosting is a boosting type of ensemble technique which can be implemented on decision trees for better predictions. XGBoost is very fast, avoids overfitting and has features for tuning the model using parameters like ‘learning_rate’, ‘n_estimators’ etc. Therefore, XGBoost is a suitable model for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Which metrics will you use to evaluate your model?\n",
    "\n",
    "We are using the following three metrics to evaluate our model.\n",
    "\n",
    "a.\tMean Absolute Error – Calculates the average magnitude of absolute error.\n",
    "b.\tMean Squared Error – The absolute error is squared, and its average’s root is used as score.\n",
    "c.\tr2_score – Determines how close the data is fitted to the actual regression value. It gives the percentage of the score, giving us an estimate on our model’s performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. How do you make sure to not overfit?\n",
    "\n",
    "We are using Cross-validation method for training Decision Trees predictive model. We are using 5 folds to get a good model, considering the limitation of computational power, and the size of the cleaned dataset. This will help us in not overfitting the Decision Tree methodology.\n",
    "\n",
    "Random Forest and XGBoost use Bagging and Boosting techniques, respectively, and therefore, do not overfit their models much. Hence, we aren’t using an explicit cross-validation for these two models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training and Testing Data | One Hot Encoding\n",
    "\n",
    "We are loading the feature attributes and label attributes into variables, and performing One Hot Encoding on the categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEiK11Oe_rdE"
   },
   "outputs": [],
   "source": [
    "# load training and test data\n",
    "X = temp_data[[\"location\",\"rest_type\",\"rate\",\"cuisines\"]]\n",
    "Y = temp_data[[\"cost\"]]\n",
    "\n",
    "location = pd.get_dummies(X['location'],prefix='location')\n",
    "X = X.drop('location',axis=1)\n",
    "X = pd.concat([X,location],axis=1)\n",
    "\n",
    "rest_type = pd.get_dummies(X['rest_type'],prefix='rest')\n",
    "X = X.drop('rest_type',axis=1)\n",
    "X = pd.concat([X,rest_type],axis=1)\n",
    "\n",
    "cuisines = pd.get_dummies(X['cuisines'],prefix='cusine')\n",
    "X = X.drop('cuisines',axis=1)\n",
    "X = pd.concat([X,cuisines],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion\n",
    "\n",
    "Converting String data types (if any) to float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "bYA0HuLPcCzV",
    "outputId": "dcbda5ec-ab17-468e-8508-6af4fff747d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3391: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "X[list(X.columns)] = X[list(X.columns)].astype(float)\n",
    "Y[list(Y.columns)] = Y[list(Y.columns)].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "We have trained the model using a Decision Tree Regressor using the data divided in the earlier steps.\n",
    "\n",
    "After training and fitting the model, we evaluated it against the following metrics and each metric returned a value.\n",
    "\n",
    "It was found that Decision Tree returned better performance than Linear Regression and SVR. As Linear Regression and SVR did not suit the problem well, we dropped using those models after a trial.\n",
    "\n",
    "The model returned a r2_score of 0.73085."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "g4W_4vaRFtr0",
    "outputId": "5e127c8a-4211-43ed-8cbd-b42d0e362e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7308575988920678\n",
      "55420.51909804613\n",
      "141.19893735822512\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "score_r2 = r2_score(y_test, y_pred)\n",
    "score_mean_squared_error = mean_squared_error(y_test, y_pred)\n",
    "score_mean_absolute_error = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(score_r2)\n",
    "print(score_mean_squared_error)\n",
    "print(score_mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "We have trained the model using a Random Forest Regressor using the data divided in the earlier steps.\n",
    "\n",
    "After training and fitting the model, we evaluated it against the following metrics and each metric returned a value.\n",
    "\n",
    "The model returned a r2_score of 0.76721."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "xXcLWVdl24Cb",
    "outputId": "99e38532-9423-446d-afbb-faeea45836ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767214115444503\n",
      "47934.15867457407\n",
      "138.56744009082092\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "score_r2 = r2_score(y_test, y_pred)\n",
    "score_mean_squared_error = mean_squared_error(y_test, y_pred)\n",
    "score_mean_absolute_error = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(score_r2)\n",
    "print(score_mean_squared_error)\n",
    "print(score_mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regressor\n",
    "\n",
    "We have trained the model using a XGBoost Regressor using the data divided in the earlier steps.\n",
    "\n",
    "After training and fitting the model, we evaluated it against the following metrics and each metric returned a value.\n",
    "\n",
    "The model returned a r2_score of 0.73035."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "MzhNI9faIp6d",
    "outputId": "611a9dc0-b358-40cf-ae31-d280d7ce54d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:01:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.7303594440475014\n",
      "55523.09676683196\n",
      "158.29385547803156\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "regressor = XGBRegressor()\n",
    "regressor.fit(X_test, y_test)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "score_r2 = r2_score(y_test, y_pred)\n",
    "score_mean_squared_error = mean_squared_error(y_test, y_pred)\n",
    "score_mean_absolute_error = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(score_r2)\n",
    "print(score_mean_squared_error)\n",
    "print(score_mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with Cross Validation\n",
    "\n",
    "We then used Cross Validation technique to train the Random Forest Regressor, and found performance of each fold. The r2_score of various models generated by different folds are listed below. We have evaluated and found that the r2_score using Cross Validation is better than that of the normal Random Forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "fg7jCJSJJFx2",
    "outputId": "d0667b4a-e0a0-4b3c-8c3d-2d8686402510",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:49:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[01:50:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[01:50:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "{'fit_time': array([24.38116956, 23.12699437, 21.97568893]), 'score_time': array([0.07738805, 0.0711062 , 0.07295012]), 'test_score': array([0.78220274, 0.78376558, 0.75871562])}\n"
     ]
    }
   ],
   "source": [
    "# cross validation for Random forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "regressor = XGBRegressor(learning_rate=0.3,n_estimators=300)\n",
    "scorer = make_scorer(r2_score)\n",
    "\n",
    "result = cross_validate(regressor,X_train,y_train,cv = 3,scoring=scorer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the XGBoost Model \n",
    "\n",
    "The XGBoost model can be tuned to perform better, and we tried using GridSearch for it. XGBoost has parameters that can be used to tune the model’s performance.\n",
    "\n",
    "We’re considering two major parameters, ‘learning_rate’ and ‘n_estimator’\n",
    "\n",
    "### Learning_rate: \n",
    "\n",
    "XGBoost models are fast, and can learn things quickly. This can sometimes lead to overfitting. In order to avoid this, we tune the learning_rate parameter. The default value is 0.1, but we can verify for various parameters to know which one gives the best result.\n",
    "\n",
    "We have tried tuning the model between 6 learning rate values, and determined that the model performs best at learning_rate=0.3 as it has the best r2_score (0.741497).\n",
    "\n",
    "### N Estimator: \n",
    "(We made a trial and error and got the below results)\n",
    "This parameter determines the number of trees used to estimate (estimators), and usually, the greater number of trees, the better is the performance. But at one time, it reaches saturation, and increasing the value doesn’t increase the performance anymore.\n",
    "\n",
    "We tried performing the prediction with three values, N= 100, N= 200 and N = 1000. There has been a significant increase in the performance as we increased the estimators.\n",
    "\n",
    "N = 100 -> r2_score = 0.7627591\n",
    "N = 200 -> r2_score = 0.7753006\n",
    "N = 1000 -> r2_score = 0.7938902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "u0udSeQJJSo8",
    "outputId": "86265637-d8ff-4106-abe7-25d035825a26",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:47:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Best: 0.741497 using {'learning_rate': 0.3}\n",
      "-5.251274 (0.076278) with: {'learning_rate': 0.0001}\n",
      "-4.312741 (0.061272) with: {'learning_rate': 0.001}\n",
      "-0.293461 (0.018149) with: {'learning_rate': 0.01}\n",
      "0.705897 (0.008327) with: {'learning_rate': 0.1}\n",
      "0.730556 (0.008113) with: {'learning_rate': 0.2}\n",
      "0.741497 (0.008865) with: {'learning_rate': 0.3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Log Loss')"
      ]
     },
     "execution_count": 345,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tuning model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "\n",
    "label_encoded_y = LabelEncoder().fit_transform(Y)\n",
    "model = XGBRegressor()\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(learning_rate, means, yerr=stds)\n",
    "pyplot.title(\"XGBoost learning_rate vs Log Loss\")\n",
    "pyplot.xlabel('learning_rate')\n",
    "pyplot.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Initially, we tried using the Decision Tree Regressor. And after using the ensemble method, Random Forest Regressor, we found that Random Forest gave better results than Decision Tree. We also tried XGBoost but it did not yield better results than Random Forest. \n",
    "\n",
    "But XGBoost is highly robust to overfitting. Hence, we tried to tune its performance by scanning the hyper parameters using GridSearch. Using the parameters learning_rate and n_estimators, we tuned the performance successfully.\n",
    "\n",
    "We also used Cross Validation on XGBoost and have provided the results above. Each fold had a decent r2_score (around 0.78) as seen above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] J. Brownlee, \"Tune Learning Rate for Gradient Boosting with XGBoost in Python\", Machine Learning Mastery, 2019. [Online]. Available: https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[2]\"Understanding Random Forest\", Towards Data Science, 2019. [Online]. Available: https://towardsdatascience.com/understanding-random-forest-58381e0602d2. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[3]\"API Reference — scikit-learn 0.21.2 documentation\", Scikit-learn.org, 2019. [Online]. Available: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[4]\"scikit-learn: machine learning in Python — scikit-learn 0.21.2 documentation\", Scikit-learn.org, 2019. [Online]. Available: https://scikit-learn.org/stable/. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[5]\"3.2. Tuning the hyper-parameters of an estimator — scikit-learn 0.21.2 documentation\", Scikit-learn.org, 2019. [Online]. Available: https://scikit-learn.org/stable/modules/grid_search.html#multimetric-grid-search. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[6]Z. Z., \"How to use pd.get_dummies() with the test set - FastML\", Fastml.com, 2019. [Online]. Available: http://fastml.com/how-to-use-pd-dot-get-dummies-with-the-test-set/. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[7]M. Editor, \"How to Choose the Best Regression Model\", Blog.minitab.com, 2019. [Online]. Available: https://blog.minitab.com/blog/how-to-choose-the-best-regression-model. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[8]\"Zomato Data Exploration and Visualization | Kaggle\", Kaggle.com, 2019. [Online]. Available: https://www.kaggle.com/agnihotri/zomato-data-exploration-and-visualization. [Accessed: 03- Jul- 2019].\n",
    "\n",
    "[9]H. Poddar, \"Zomato Bangalore Restaurants\", Kaggle.com, 2019. [Online]. Available: https://www.kaggle.com/himanshupoddar/zomato-bangalore-restaurants. [Accessed: 03- Jul- 2019]."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ASsignment1",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
